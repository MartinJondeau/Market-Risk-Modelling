{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fd458f9",
   "metadata": {},
   "source": [
    "### **Question E | Multiresolution Correlation, Hurst Exponent and Annualized Volatility**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea03256",
   "metadata": {},
   "source": [
    "> 1. With Haar wavelets and the dataset provided with TD5, determine the multiresolution correlation\n",
    "between all the pairs of FX rates, using GBPEUR, SEKEUR, and CADEUR (work with the average between\n",
    "the highest and the lowest price and transform this average price in returns on the smallest time step).\n",
    "Do you observe an Epps effect and how could you explain this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c10135c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GBPEUR Curncy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SEKEUR Curncy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CADEUR Curncy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Date</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Date</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Date</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>LOW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0     1    2   3              4     5    6   7              8   \\\n",
       "0            NaN   NaN  NaN NaN            NaN   NaN  NaN NaN            NaN   \n",
       "1  GBPEUR Curncy   NaN  NaN NaN  SEKEUR Curncy   NaN  NaN NaN  CADEUR Curncy   \n",
       "2           Date  HIGH  LOW NaN           Date  HIGH  LOW NaN           Date   \n",
       "\n",
       "     9    10  \n",
       "0   NaN  NaN  \n",
       "1   NaN  NaN  \n",
       "2  HIGH  LOW  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open original data file\n",
    "import pandas as pd\n",
    "raw_data = pd.read_excel(\"Dataset TD5.xlsx\", header=None)\n",
    "raw_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cc2fd91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high_GBPEUR</th>\n",
       "      <th>low_GBPEUR</th>\n",
       "      <th>high_SEKEUR</th>\n",
       "      <th>low_SEKEUR</th>\n",
       "      <th>high_CADEUR</th>\n",
       "      <th>low_CADEUR</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.2932</td>\n",
       "      <td>1.2917</td>\n",
       "      <td>0.10725</td>\n",
       "      <td>0.1072</td>\n",
       "      <td>0.6842</td>\n",
       "      <td>0.6829</td>\n",
       "      <td>2016-03-07 08:59:59.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.294</td>\n",
       "      <td>1.293</td>\n",
       "      <td>0.10728</td>\n",
       "      <td>0.10717</td>\n",
       "      <td>0.6849</td>\n",
       "      <td>0.6841</td>\n",
       "      <td>2016-03-07 09:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.2943</td>\n",
       "      <td>1.2922</td>\n",
       "      <td>0.10726</td>\n",
       "      <td>0.10719</td>\n",
       "      <td>0.6844</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>2016-03-07 09:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.293</td>\n",
       "      <td>1.2913</td>\n",
       "      <td>0.10728</td>\n",
       "      <td>0.10721</td>\n",
       "      <td>0.6844</td>\n",
       "      <td>0.6839</td>\n",
       "      <td>2016-03-07 09:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.2931</td>\n",
       "      <td>1.2921</td>\n",
       "      <td>0.10725</td>\n",
       "      <td>0.10719</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.6835</td>\n",
       "      <td>2016-03-07 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12927</th>\n",
       "      <td>1.1879</td>\n",
       "      <td>1.1867</td>\n",
       "      <td>0.10536</td>\n",
       "      <td>0.10531</td>\n",
       "      <td>0.6897</td>\n",
       "      <td>0.6893</td>\n",
       "      <td>2016-09-07 17:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12928</th>\n",
       "      <td>1.1883</td>\n",
       "      <td>1.1874</td>\n",
       "      <td>0.10537</td>\n",
       "      <td>0.10534</td>\n",
       "      <td>0.6902</td>\n",
       "      <td>0.6895</td>\n",
       "      <td>2016-09-07 17:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12929</th>\n",
       "      <td>1.188</td>\n",
       "      <td>1.1874</td>\n",
       "      <td>0.10538</td>\n",
       "      <td>0.10536</td>\n",
       "      <td>0.6902</td>\n",
       "      <td>0.6898</td>\n",
       "      <td>2016-09-07 17:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12930</th>\n",
       "      <td>1.1874</td>\n",
       "      <td>1.1866</td>\n",
       "      <td>0.10537</td>\n",
       "      <td>0.10536</td>\n",
       "      <td>0.6902</td>\n",
       "      <td>0.6901</td>\n",
       "      <td>2016-09-07 17:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12931</th>\n",
       "      <td>1.187</td>\n",
       "      <td>1.1869</td>\n",
       "      <td>0.10537</td>\n",
       "      <td>0.10537</td>\n",
       "      <td>0.6901</td>\n",
       "      <td>0.6901</td>\n",
       "      <td>2016-09-07 18:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12929 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      high_GBPEUR low_GBPEUR high_SEKEUR low_SEKEUR high_CADEUR low_CADEUR  \\\n",
       "3          1.2932     1.2917     0.10725     0.1072      0.6842     0.6829   \n",
       "4           1.294      1.293     0.10728    0.10717      0.6849     0.6841   \n",
       "5          1.2943     1.2922     0.10726    0.10719      0.6844     0.6837   \n",
       "6           1.293     1.2913     0.10728    0.10721      0.6844     0.6839   \n",
       "7          1.2931     1.2921     0.10725    0.10719       0.684     0.6835   \n",
       "...           ...        ...         ...        ...         ...        ...   \n",
       "12927      1.1879     1.1867     0.10536    0.10531      0.6897     0.6893   \n",
       "12928      1.1883     1.1874     0.10537    0.10534      0.6902     0.6895   \n",
       "12929       1.188     1.1874     0.10538    0.10536      0.6902     0.6898   \n",
       "12930      1.1874     1.1866     0.10537    0.10536      0.6902     0.6901   \n",
       "12931       1.187     1.1869     0.10537    0.10537      0.6901     0.6901   \n",
       "\n",
       "                             date  \n",
       "3      2016-03-07 08:59:59.990000  \n",
       "4             2016-03-07 09:15:00  \n",
       "5             2016-03-07 09:30:00  \n",
       "6             2016-03-07 09:45:00  \n",
       "7             2016-03-07 10:00:00  \n",
       "...                           ...  \n",
       "12927         2016-09-07 17:00:00  \n",
       "12928         2016-09-07 17:15:00  \n",
       "12929         2016-09-07 17:30:00  \n",
       "12930         2016-09-07 17:45:00  \n",
       "12931         2016-09-07 18:00:00  \n",
       "\n",
       "[12929 rows x 7 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We fix the data format by handpicking columns to make the dataframe we want\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# For pound - euro\n",
    "df[\"high_GBPEUR\"] = raw_data.iloc[3:, 1] # 2nd column starting from 4th row\n",
    "df[\"low_GBPEUR\"]  = raw_data.iloc[3:, 2] # 3rd column starting from 4th row\n",
    "\n",
    "# For sweden crown - euro\n",
    "df[\"high_SEKEUR\"] = raw_data.iloc[3:, 5] # 6th column starting from 4th row\n",
    "df[\"low_SEKEUR\"]  = raw_data.iloc[3:, 6] # 7th column starting from 4th row\n",
    "\n",
    "# For canada dollar - euro\n",
    "df[\"high_CADEUR\"] = raw_data.iloc[3:, 9] # 10th column starting from 4th row\n",
    "df[\"low_CADEUR\"]  = raw_data.iloc[3:, 10] # 11th column starting from 4th row\n",
    "\n",
    "# Then we add the date \n",
    "df[\"date\"] = raw_data.iloc[3:, 0] # 1st column starting from 4th row\n",
    "# Fixed data\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "20430ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adrien\\AppData\\Local\\Temp\\ipykernel_17736\\4129758546.py:4: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['returns_GBPEUR']=df['mid_GBPEUR'].pct_change()\n",
      "C:\\Users\\Adrien\\AppData\\Local\\Temp\\ipykernel_17736\\4129758546.py:6: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['returns_SEKEUR']=df['mid_SEKEUR'].pct_change()\n",
      "C:\\Users\\Adrien\\AppData\\Local\\Temp\\ipykernel_17736\\4129758546.py:8: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['returns_CADEUR']=df['mid_CADEUR'].pct_change()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high_GBPEUR</th>\n",
       "      <th>low_GBPEUR</th>\n",
       "      <th>high_SEKEUR</th>\n",
       "      <th>low_SEKEUR</th>\n",
       "      <th>high_CADEUR</th>\n",
       "      <th>low_CADEUR</th>\n",
       "      <th>date</th>\n",
       "      <th>mid_GBPEUR</th>\n",
       "      <th>returns_GBPEUR</th>\n",
       "      <th>mid_SEKEUR</th>\n",
       "      <th>returns_SEKEUR</th>\n",
       "      <th>mid_CADEUR</th>\n",
       "      <th>returns_CADEUR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.294</td>\n",
       "      <td>1.293</td>\n",
       "      <td>0.10728</td>\n",
       "      <td>0.10717</td>\n",
       "      <td>0.6849</td>\n",
       "      <td>0.6841</td>\n",
       "      <td>2016-03-07 09:15:00</td>\n",
       "      <td>1.2935</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.107225</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.6845</td>\n",
       "      <td>1.389803e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.2943</td>\n",
       "      <td>1.2922</td>\n",
       "      <td>0.10726</td>\n",
       "      <td>0.10719</td>\n",
       "      <td>0.6844</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>2016-03-07 09:30:00</td>\n",
       "      <td>1.29325</td>\n",
       "      <td>-0.000193</td>\n",
       "      <td>0.107225</td>\n",
       "      <td>-1.110223e-16</td>\n",
       "      <td>0.68405</td>\n",
       "      <td>-6.574142e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.293</td>\n",
       "      <td>1.2913</td>\n",
       "      <td>0.10728</td>\n",
       "      <td>0.10721</td>\n",
       "      <td>0.6844</td>\n",
       "      <td>0.6839</td>\n",
       "      <td>2016-03-07 09:45:00</td>\n",
       "      <td>1.29215</td>\n",
       "      <td>-0.000851</td>\n",
       "      <td>0.107245</td>\n",
       "      <td>1.865237e-04</td>\n",
       "      <td>0.68415</td>\n",
       "      <td>1.461881e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.2931</td>\n",
       "      <td>1.2921</td>\n",
       "      <td>0.10725</td>\n",
       "      <td>0.10719</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.6835</td>\n",
       "      <td>2016-03-07 10:00:00</td>\n",
       "      <td>1.2926</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.10722</td>\n",
       "      <td>-2.331111e-04</td>\n",
       "      <td>0.68375</td>\n",
       "      <td>-5.846671e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.2926</td>\n",
       "      <td>1.2921</td>\n",
       "      <td>0.10724</td>\n",
       "      <td>0.10718</td>\n",
       "      <td>0.6839</td>\n",
       "      <td>0.6836</td>\n",
       "      <td>2016-03-07 10:15:00</td>\n",
       "      <td>1.29235</td>\n",
       "      <td>-0.000193</td>\n",
       "      <td>0.10721</td>\n",
       "      <td>-9.326618e-05</td>\n",
       "      <td>0.68375</td>\n",
       "      <td>-1.110223e-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  high_GBPEUR low_GBPEUR high_SEKEUR low_SEKEUR high_CADEUR low_CADEUR  \\\n",
       "4       1.294      1.293     0.10728    0.10717      0.6849     0.6841   \n",
       "5      1.2943     1.2922     0.10726    0.10719      0.6844     0.6837   \n",
       "6       1.293     1.2913     0.10728    0.10721      0.6844     0.6839   \n",
       "7      1.2931     1.2921     0.10725    0.10719       0.684     0.6835   \n",
       "8      1.2926     1.2921     0.10724    0.10718      0.6839     0.6836   \n",
       "\n",
       "                  date mid_GBPEUR  returns_GBPEUR mid_SEKEUR  returns_SEKEUR  \\\n",
       "4  2016-03-07 09:15:00     1.2935        0.000812   0.107225    0.000000e+00   \n",
       "5  2016-03-07 09:30:00    1.29325       -0.000193   0.107225   -1.110223e-16   \n",
       "6  2016-03-07 09:45:00    1.29215       -0.000851   0.107245    1.865237e-04   \n",
       "7  2016-03-07 10:00:00     1.2926        0.000348    0.10722   -2.331111e-04   \n",
       "8  2016-03-07 10:15:00    1.29235       -0.000193    0.10721   -9.326618e-05   \n",
       "\n",
       "  mid_CADEUR  returns_CADEUR  \n",
       "4     0.6845    1.389803e-03  \n",
       "5    0.68405   -6.574142e-04  \n",
       "6    0.68415    1.461881e-04  \n",
       "7    0.68375   -5.846671e-04  \n",
       "8    0.68375   -1.110223e-16  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we compute the average between the lowest and the highest price\n",
    "# We do the operation for each fx rate\n",
    "df['mid_GBPEUR'] = (df['high_GBPEUR']+df['low_GBPEUR'])/2\n",
    "df['returns_GBPEUR']=df['mid_GBPEUR'].pct_change()\n",
    "df['mid_SEKEUR'] = (df['high_SEKEUR']+df['low_SEKEUR'])/2\n",
    "df['returns_SEKEUR']=df['mid_SEKEUR'].pct_change()\n",
    "df['mid_CADEUR'] = (df['high_CADEUR']+df['low_CADEUR'])/2\n",
    "df['returns_CADEUR']=df['mid_CADEUR'].pct_change()\n",
    "df = df.drop(3, axis=0) # We remove the line with NaNs\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28075e14",
   "metadata": {},
   "source": [
    "Now that the data is formatted in a practical way, we can calculate the Haar wavelet transforms to then analyze multiresolution correlations between all the FX rates.\n",
    "\n",
    "Haar wavelets are useful to approximate functions, and they follow the expression:\n",
    "$$\\psi(t) =\n",
    "\\begin{cases}\n",
    "-1, & 0 \\le t < \\tfrac{1}{2}, \\\\\n",
    "\\;\\;1, & \\tfrac{1}{2} \\le t < 1, \\\\\n",
    "0, & \\text{otherwise}.\n",
    "\\end{cases}$$\n",
    "\n",
    "The bigger the step, the more we approximate the true shape of the function, it's like a blurry filter.\n",
    "\n",
    "Originally, we thought the method to execute a Haar wavelet transformation used a sort of rolling window (overlap), but according to the Wikipedia page on wavelet transforms, the classic method for calculating specifically the *Haar* wavelets does not use overlap (https://en.wikipedia.org/wiki/Discrete_wavelet_transform), so we won't use overlap and will \"lose\" half the data at each time frame increment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cb14520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Function for transforming the return arrays in wavelets for different time steps (15', 30', 60' etc) \n",
    "def haar_transformation(data):\n",
    "    # put the returns data into a numpy array\n",
    "    a = data.values\n",
    "    coef = []\n",
    "    \n",
    "    # We loop as long as possible, so until the size of the scale is half the dataset\n",
    "    while len(a) >= 2:\n",
    "        # If odd length, drop the last one to ensure pairing\n",
    "        if len(a) % 2 != 0:\n",
    "            a = a[:-1]\n",
    "            \n",
    "        # We take steps of 2 every time, allowing us to calculate all differences and approximations at once\n",
    "        # even = a[0], a[2], a[4], etc\n",
    "        # odd  = a[1], a[3], a[5], etc\n",
    "        # Here we got two arrays containing the [0, 1/2[ and [1/2, 1[ of each step\n",
    "        even = a[0::2]\n",
    "        odd  = a[1::2]\n",
    "        \n",
    "        # First we start by storing the difference\n",
    "        d = (even - odd) / np.sqrt(2)\n",
    "        coef.append(d)\n",
    "        \n",
    "        # Second, we pass the sum array to the next level\n",
    "        # This becomes the input 'a' for the next iteration\n",
    "        next_a = (even + odd) / np.sqrt(2)\n",
    "        a = next_a\n",
    "        \n",
    "    # To finish we also store the last remaining sum, which represents the overall trend of the series\n",
    "    coef.append(a) \n",
    "    # we have coef the list of all the haar transformations arrays of the original series, with indices j the different time scales\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22109d6c",
   "metadata": {},
   "source": [
    "Now we want to implement a function to calculate covariances between the FX rates at different time scales. \n",
    "\n",
    "The lesson states the covariance of the scale coefficients for a given scale $j$ is:\n",
    "\n",
    "$$\\mathrm{Cov}(j)\n",
    "=\n",
    "\\frac{1}{T}\n",
    "\\sum_{k=1}^{T}\n",
    "\\left(\n",
    "c_{j,k}^{1}\n",
    "- \\frac{1}{T}\\sum_{l=1}^{T} c_{j,l}^{1}\n",
    "\\right)\n",
    "\\left(\n",
    "c_{j,k}^{2}\n",
    "- \\frac{1}{T}\\sum_{l=1}^{T} c_{j,l}^{2}\n",
    "\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1b1b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the covariance between two Haar transformed series for each time scale j\n",
    "def covariance(coef1, coef2):\n",
    "    # even out number of steps\n",
    "    min_levels = min(len(coef1), len(coef2))\n",
    "    covariances = []    \n",
    "    # We iterate on each time scale j\n",
    "    for j in range(min_levels):\n",
    "        c1_j = coef1[j]\n",
    "        c2_j = coef2[j]\n",
    "        \n",
    "        # Check for size and cut to the smallest size if uneven\n",
    "        t = min(len(c1_j), len(c2_j))\n",
    "        c1_j = c1_j[:t]\n",
    "        c2_j = c2_j[:t]\n",
    "        # First we calculate the means of c1 and c2\n",
    "        mean1 = np.mean(c1_j)\n",
    "        mean2 = np.mean(c2_j)\n",
    "         # after that we apply the covariance formula\n",
    "        sum_prod = np.sum((c1_j - mean1) * (c2_j - mean2))\n",
    "        # and divide everything by T\n",
    "        cov_value = sum_prod / t\n",
    "        covariances.append(cov_value)\n",
    "    # It returns a series with the covariances for all the time scales organized in increasing order of j\n",
    "    return pd.Series(covariances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4a1907",
   "metadata": {},
   "source": [
    "We want to correct the bias in the covariances, and for that we use the correlation, which is just the covariance normalized with the variances:\n",
    "$$\\rho_j = \\frac{\\text{Cov}_j(X, Y)}{\\sigma_{X,j} \\cdot \\sigma_{Y,j}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf03f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(coef1, coef2):\n",
    "    \n",
    "    # at the numerator we calculate the cov between the two series for each j\n",
    "    cov_12 = covariance(coef1, coef2)\n",
    "    \n",
    "    # At the denominator we calculate standard deviations of each series for all j\n",
    "    # We use cov(X,X) which is the variance itself\n",
    "    var_1 = covariance(coef1, coef1)\n",
    "    var_2 = covariance(coef2, coef2)\n",
    "    \n",
    "    # The std is sqrt(var)\n",
    "    sigma_1 = np.sqrt(var_1)\n",
    "    sigma_2 = np.sqrt(var_2)\n",
    "    \n",
    "    # we put together numerator and denominator, and calculate a corr for each j\n",
    "    correls = cov_12 / (sigma_1 * sigma_2)\n",
    "    # Organized series of correlations for each time scale j\n",
    "    return correls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a9c848",
   "metadata": {},
   "source": [
    "> 2. Calculate the Hurst exponent of GBPEUR, SEKEUR, and CADEUR. Determine their annualized\n",
    "volatility using the daily volatility and Hurst exponents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
